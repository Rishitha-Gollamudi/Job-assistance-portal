{
    "data": [
        [
            "California Coast Credit",
            "FULLTIME",
            "Data Engineer",
            "https://www.indeed.com/viewjob?jk=cb35cd64ecfbfc8f",
            "EMPLOYER WEBSITE ADVERTISEMENT\n\nEMPLOYER: California Coast Credit Union\n\nPOSITION TITLE: Data Engineer\n\nPOSITION DUTIES: The position of Data Engineer is responsible for programming, creating logics and using code as a standard part of building, updating the database. This role is also responsible for adjusting existing code to customize database features. This role maintains the daily activities of the team responsible for adjusting existing code to customize database features. This role maintains the daily activities of the team responsible for design, implementation, maintenance and support of enterprise Data Warehouse systems. The Data Engineer provides the evaluation, implementation, maintenance and documentation of existing and emerging database technologies primarily relational, structured databases and also NoSQL, unstructured cloud database technologies. Furthermore, this role is responsible for configuring and troubleshooting database and database related applications and consulting with development and engineering resources to ensure best practices are followed. This position is also accountable for providing development and operational support to other IT development resources.\n\nMINIMUM EDUCATION AND EXPERIENCE REQUIREMENTS: Master\u2019s Degree in Electrical Engineering, or related field and five (5) years of experience as a Software Engineer, or related.\n\nSPECIAL REQUIREMENTS:\n\nEducation or experience with the following:\n\n1. Analyzing, designing, coding, debugging, testing, and modifying new software or enhancements while considering software abilities.\n\n2. Providing architecture guidance and develop specifications to resolve software problems.\n\n3. SQL, Python, Oracle Business Intelligence\n\n4. Azure Databricks and Azure Datafactory\n\nJOB LOCATION: 9201 Spectrum Center Blvd., San Diego, CA 92123\n\nSALARY RANGE: $143,250 to $173,854 per year",
            "2022-12-09T00:52:05.000Z",
            [
                "MINIMUM EDUCATION AND EXPERIENCE REQUIREMENTS: Master\u2019s Degree in Electrical Engineering, or related field and five (5) years of experience as a Software Engineer, or related",
                "Analyzing, designing, coding, debugging, testing, and modifying new software or enhancements while considering software abilities",
                "Providing architecture guidance and develop specifications to resolve software problems",
                "SQL, Python, Oracle Business Intelligence"
            ],
            [
                "SALARY RANGE: $143,250 to $173,854 per year"
            ]
        ],
        [
            "Booz Allen Hamilton",
            "FULLTIME",
            "Data Engineer, Lead",
            "https://careers.boozallen.com/jobs/JobDetail/San-Diego-Data-Engineer-Lead-R0156684/69562",
            "Data Engineer, Lead\n\nThe Opportunity:\n\nEver-expanding technology like IoT, machine learning, and artificial intelligence means that there\u2019s more structured and unstructured data available today than ever before. As a data engineer, you know that organizing big data can yield pivotal insights when it\u2019s gathered from disparate sources. We need a big data experienced professional like you to help our clients find answers in their data to impact important missions from fraud detection to cancer research, to national intelligence.\n\nAs a lead big data engineer at Booz Allen, you\u2019ll use your expertise to lead data engineering activities on some of the most mission-driven projects in the industry. You\u2019ll oversee the development and deployment of pipelines and platforms that organize and make disparate data meaningful.\n\nHere, you\u2019ll guide and mentor a multi-disciplinary team of analysts, data engineers, developers, and data consumers in a fast-paced, agile environment. You\u2019ll use your expertise in analytical exploration and data examination while you oversee the assessment, design, building, and maintenance of scalable platforms for your clients.\n\nWork with us to use big data for good.\n\nJoin us. The world can\u2019t wait.\n\nYou Have:\u202f\n\u2022 5+ years of experience with data engineering, data architecture, machine learning engineering, or software engineering\n\u2022 5+ years of experience designing, developing, operationalizing, and maintaining complex data applications at enterprise scale\n\u2022 3+ years of experience creating software for retrieving, parsing, and processing structured and unstructured data\n\u2022 2+ years of experience in a management or advisory role\n\u2022 Experience building scalable ETL/ELT workflows for reporting and analytics\n\u2022 Experience creating solutions within a collaborative, cross-functional team environment\n\u2022 Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor, and operate data platforms\n\u2022 Ability to obtain a security clearance\n\u2022 Bachelor\u2019s degree\n\nNice If You Have:\u202f\n\u2022 Experience with Python, SQL, Scala, or Java\n\u2022 Experience with UNIX and Linux, including basic commands and Shell scripting\n\u2022 Experience with a public cloud, including AWS, Microsoft Azure, or Google Cloud\n\u2022 Experience with distributed data and computing tools, including Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka\n\u2022 Experience working on real-time data and streaming applications\n\u2022 Experience with NoSQL implementation, including MongoDB or Cassandra\n\u2022 Experience with data warehousing using AWS Redshift, MySQL, or Snowflake\n\u2022 Experience with Agile engineering practices\n\nClearance:\n\nApplicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.\n\nCreate Your Career:\n\nGrow With Us\n\nYour growth matters to us\u2014that\u2019s why we offer a variety of ways for you to develop your career. With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms.\n\nA Place Where You Belong\n\nDiverse perspectives cultivate collective ingenuity. Booz Allen\u2019s culture of respect, equity, and opportunity means that, here, you are free to bring your whole self to work. With an array of business resource groups and other opportunities for connection, you\u2019ll develop your community in no time.\n\nSupport Your Well-Being\n\nOur comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more. With these benefits, plus the option for flexible schedules and remote and hybrid locations, we\u2019ll support you as you pursue a balanced, fulfilling life\u2014at work and at home.\n\nYour Candidate Journey\n\nAt Booz Allen, we know our people are what propel us forward, and we value relationships most of all. Here, we\u2019ve compiled a list of resources so you\u2019ll know what to expect as we forge a connection with you during your journey as a candidate with us.\n\nCompensation\n\nAt Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. Our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. Full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in Booz Allen\u2019s benefit programs. Individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. We encourage you to learn more about our total benefits by visiting the Resource page on our Careers site and reviewing Our Employee Benefits page.\n\nSalary at Booz Allen is determined by various factors, including but not limited to location, the individual\u2019s particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. The projected compensation range for this position is $93,300.00 to $212,000.00 (annualized USD). The estimate displayed represents the typical salary range for this position and is just one component of Booz Allen\u2019s total compensation package for employees.\n\nWork Model\nOur people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely.\n\u2022 If this position is listed as remote or hybrid, you\u2019ll periodically work from a Booz Allen or client site facility.\n\u2022 If this position is listed as onsite, you\u2019ll work with colleagues and clients in person, as needed for the specific role.\n\nEEO Commitment\n\nWe\u2019re an equal employment opportunity/affirmative action employer that empowers our people to fearlessly drive change \u2013 no matter their race, color, ethnicity, religion, sex (including pregnancy, childbirth, lactation, or related medical conditions), national origin, ancestry, age, marital status, sexual orientation, gender identity and expression, disability, veteran status, military or uniformed service member status, genetic information, or any other status protected by applicable federal, state, local, or international law.",
            "2022-11-02T00:00:00.000Z",
            [
                "5+ years of experience with data engineering, data architecture, machine learning engineering, or software engineering",
                "5+ years of experience designing, developing, operationalizing, and maintaining complex data applications at enterprise scale",
                "3+ years of experience creating software for retrieving, parsing, and processing structured and unstructured data",
                "2+ years of experience in a management or advisory role",
                "Experience building scalable ETL/ELT workflows for reporting and analytics",
                "Experience creating solutions within a collaborative, cross-functional team environment",
                "Ability to develop scripts and programs for converting various types of data into usable formats and support project team to scale, monitor, and operate data platforms",
                "Ability to obtain a security clearance",
                "Bachelor\u2019s degree",
                "Experience with Python, SQL, Scala, or Java",
                "Experience with UNIX and Linux, including basic commands and Shell scripting",
                "Experience with a public cloud, including AWS, Microsoft Azure, or Google Cloud",
                "Experience with distributed data and computing tools, including Spark, Databricks, Hadoop, Hive, AWS EMR, or Kafka",
                "Experience working on real-time data and streaming applications",
                "Experience with NoSQL implementation, including MongoDB or Cassandra",
                "Experience with data warehousing using AWS Redshift, MySQL, or Snowflake",
                "Experience with Agile engineering practices"
            ],
            [
                "With professional and leadership development opportunities like upskilling programs, tuition reimbursement, mentoring, and firm-sponsored networking, you can chart a unique and fulfilling career path on your own terms",
                "Our comprehensive benefits package includes wellness programs with HSA contributions, paid holidays, paid parental leave, a generous 401(k) match, and more",
                "With these benefits, plus the option for flexible schedules and remote and hybrid locations, we\u2019ll support you as you pursue a balanced, fulfilling life\u2014at work and at home",
                "At Booz Allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being",
                "Our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care",
                "The projected compensation range for this position is $93,300.00 to $212,000.00 (annualized USD)"
            ]
        ],
        [
            "San Diego State University Research Foundation",
            "FULLTIME",
            "KPBS Data Engineer",
            "https://www.monster.com/job-openings/kpbs-data-engineer-san-diego-ca--c08e1e3b-2af8-494e-9e41-0efe344199bf",
            "Overview\n\nUnder the general direction of the Director of Digital, the Data Engineer is primarily responsible for integrating data from various CRM applications (iMIS (CRM), Eventbrite, Google Ads, Google Analytics, PBS, NPR) and other data sources into the software data warehouse (Snowflake) and customer data platform (Lytics).\n\nThis position will assist the Data Analyst in maintaining databases that house marketing and analysis data; working with subject matter experts in various departments at KPBS, working with staff on data creation policies and procedures and performing data quality checks, as needed. The position will be the lead on problem-solving technical issues within the Snowflake Data Warehouse and will work across departments to ensure the effectiveness of Key Performance Indicators within the organization.\n\nThe Data Engineer will provide program implementation and policy creation guidance and support. This includes responsibility for the monitoring of data quality, data governance, data access, data cleaning, data validity and timely consumption and delivery of the data to and from various endpoints.\n\nPerks you'll enjoy as a member of #teamKPBS\n\n- Working on a college campus & in public media (access to campus facilities and staff discounts, farmer's market Tuesday's, tons of eateries, community events, entertainment)\n\n- Hybrid schedule, remote work flexibility & casual work attire\n\n- Paid time off on your birthday (can take on any day)\n\n- 10 vacation days and 13 holidays off (enjoy 4+ days off over winter break)\n\n- Enjoy a 'beautiful day in the neighborhood' with local employee discounts\n\n- Opportunity drawings to attend SDSU basketball games, local concerts, and events\n\n- Free wellness classes & programs\n\n- Monthly pop-up events for staff & KPBS swag\n\n- Full benefits packages that are unmatched (medical, dental, vision, life)\n\n- Sick leave accruals and paid leave options\n\n- On-site childcare at a discounted rate\n\n- Opportunities for continued learning and professional development\n\n- Flexible spending account(s)\n\n- Employee assistance program\n\n- Matching and voluntary retirement savings plan\n\nSalary range is $7,083.33/mo - $10,833.33/mo ($85,000 - $130,000/yr).\nResponsibilities\n\nThe following information is intended to be representative of the work performed by incumbents in this position and is not all-inclusive. The omission of a specific duty or responsibility will not preclude it from the position if the work is similar, related, or a logical extension of position responsibilities. Job descriptions may be changed at any time based on the needs of the department.\n\nData Management & Development (35%)\n\u2022 Build, manage and maintain data pipelines for data integration (ELT/ ETL)\n\u2022 Perform data quality checks and clean ups at various data collection and storage touchpoints\n\u2022 Ensure security of data during transit and at rest\n\u2022 Transform data for use in marketing and analytics applications\n\nData Administration (35%)\n\u2022 Perform as the lead administrator role for the Snowflake Data Warehouse\n\u2022 Act as the lead for problem-solving technical issues within or related to the Data Warehouse\n\u2022 Work across departments to ensure effective data usage in the organization\n\u2022 Create and maintain data policies that will ensure accuracy of reports and usage of data for marketing\n\nProject Implementation (15%)\n\u2022 Work across departments to gather requirements, implement various projects as they arise\n\nSupport and Staff Training (10%)\n\u2022 Support Data Analyst by providing data for use in analysis, managing connections to BI applications, dashboard creation, etc.\n\u2022 Lead periodic trainings for Snowflake users\n\u2022 Train support staff on data creation policies and procedures, business process creation and other activities that impact the quality and usefulness of data collected\n\u2022 Convene and lead workgroups, create/implement policies and monitor and oversee the data quality across multiple systems\n\nOther Duties as Assigned (5%)\nQualifications\n\nMINIMUM EDUCATION\n\u2022 Bachelor\u2019s degree in computer science, information systems, business administration or similar field. Additional or equivalent years\u2019 of work experience may be substituted for the required education on a year for year basis.\n\nMINIMUM EXPERIENCE\n\u2022 Four-years of related work experience in data engineering; developing data pipelines\n\nPREFERRED EXPERIENCE\n\u2022 2-3 years of which involve SQL database administration;\n\u2022 3-4 years of which include experience developing, maintaining and administering software applications on the linux operating system\n\nKNOWLEDGE & ABILITIES\n\u2022 Demonstrated experience with data integration toolsets (i.e Airflow)\n\u2022 Demonstrated experience with writing and maintaining Data Pipelines\n\u2022 Demonstrated ability of good scripting, including Bash scripting and Python\n\u2022 Demonstrated experience with cloud technologies such as AWS, GCP, Azure\n\u2022 Demonstrated experience designing and working on complex data systems from design to delivery\n\u2022 Knowledge of and familiarity with Data Modeling techniques such as dimensional modeling and Data Warehousing standard methodologies and practices\n\u2022 Knowledge of SQL and ability to create queries to extract data and build performant datasets\n\u2022 Demonstrated expertise in using Snowflake cloud data warehouse including administration of roles, security, stages, etc.\n\u2022 Ability to integrate with 3rd party APIs\n\u2022 Ability to problem solve with strong attention to detail; excellent analytical and communication skills\n\u2022 Ability to work individually and as a part of a team\n\u2022 Ability to interact effectively with co-workers, vendors and consultants\n\u2022 Ability to understand and follow posted work rules and procedures\n\u2022 Ability to relate well to others within the project environment.\n\u2022 Ability to display motivation and strong interpersonal skills\n\u2022 Ability to communicate effectively, both orally and in writing\n\u2022 Ability to display organization, meet deadlines, display detail orientation, possess good judgment and common sense\n\u2022 Ability to demonstrate a high level of cross-cultural sensitivity\n\nADDITIONAL APPLICANT INFORMATION:\n\u2022 The COVID-19 vaccine is required by the CSU for all SDSU Research Foundation employees as a condition of employment. Should you be offered a position, you will be required to provide proof of vaccination status. Individuals who obtain an approved medical or religious exemption on file will be required to complete regular COVID-19 testing\n\u2022 A background check (including a criminal records check) and Livescan (fingerprint) must be completed satisfactorily before any candidate can be offered a position with SDSU Research Foundation/KPBS\n\u2022 San Diego State University Research Foundation is an EEO/AA/Disability/Vets Employer",
            "2023-04-07T15:19:53.000Z",
            [
                "Bachelor\u2019s degree in computer science, information systems, business administration or similar field",
                "Additional or equivalent years\u2019 of work experience may be substituted for the required education on a year for year basis",
                "Four-years of related work experience in data engineering; developing data pipelines",
                "Demonstrated experience with data integration toolsets (i.e Airflow)",
                "Demonstrated experience with writing and maintaining Data Pipelines",
                "Demonstrated ability of good scripting, including Bash scripting and Python",
                "Demonstrated experience with cloud technologies such as AWS, GCP, Azure",
                "Demonstrated experience designing and working on complex data systems from design to delivery",
                "Knowledge of and familiarity with Data Modeling techniques such as dimensional modeling and Data Warehousing standard methodologies and practices",
                "Knowledge of SQL and ability to create queries to extract data and build performant datasets",
                "Demonstrated expertise in using Snowflake cloud data warehouse including administration of roles, security, stages, etc",
                "Ability to integrate with 3rd party APIs",
                "Ability to problem solve with strong attention to detail; excellent analytical and communication skills",
                "Ability to work individually and as a part of a team",
                "Ability to interact effectively with co-workers, vendors and consultants",
                "Ability to understand and follow posted work rules and procedures",
                "Ability to relate well to others within the project environment",
                "Ability to display motivation and strong interpersonal skills",
                "Ability to communicate effectively, both orally and in writing",
                "Ability to display organization, meet deadlines, display detail orientation, possess good judgment and common sense",
                "Ability to demonstrate a high level of cross-cultural sensitivity",
                "Individuals who obtain an approved medical or religious exemption on file will be required to complete regular COVID-19 testing",
                "A background check (including a criminal records check) and Livescan (fingerprint) must be completed satisfactorily before any candidate can be offered a position with SDSU Research Foundation/KPBS"
            ],
            [
                "Working on a college campus & in public media (access to campus facilities and staff discounts, farmer's market Tuesday's, tons of eateries, community events, entertainment)",
                "Hybrid schedule, remote work flexibility & casual work attire",
                "Paid time off on your birthday (can take on any day)",
                "10 vacation days and 13 holidays off (enjoy 4+ days off over winter break)",
                "Enjoy a 'beautiful day in the neighborhood' with local employee discounts",
                "Opportunity drawings to attend SDSU basketball games, local concerts, and events",
                "Free wellness classes & programs",
                "Monthly pop-up events for staff & KPBS swag",
                "Full benefits packages that are unmatched (medical, dental, vision, life)",
                "Sick leave accruals and paid leave options",
                "On-site childcare at a discounted rate",
                "Opportunities for continued learning and professional development",
                "Flexible spending account(s)",
                "Employee assistance program",
                "Matching and voluntary retirement savings plan",
                "Salary range is $7,083.33/mo - $10,833.33/mo ($85,000 - $130,000/yr)"
            ]
        ],
        [
            "UnitedHealth Group",
            "FULLTIME",
            "Sr data engineer",
            "https://www.talent.com/view?id=d3be9f56f6f3",
            "At UnitedHealthcare, we\u2019re simplifying the health care experience, creating healthier communities and removing barriers to quality care.\n\nThe work you do here impacts the lives of millions of people for the better. Come build the health care system of tomorrow, making it more responsive, affordable and equitable.\n\nReady to make a difference? Join us and start doing your life's best work.(sm)\n\nOur vision is to use data and insights to design, orchestrate and maximize experiences that matter to consumers, clients, and health care professionals, resulting in enterprise strategic growth as well as a brand that people love.\n\nTo support this vision, one of our goals is to evolve our analytic and data environments to improve speed of decisioning and to support higher velocity use cases.\n\nWe are looking for someone who is passionate about complex data structures and problem-solving skills and someone who wants to come along for the ride and grow with us to support building a new analytics warehouse in cloud and implement efficient and robust ETL / ELT processes.\n\nYou will build data pipelines to ingest the data from heterogeneous sources into our system Hadoop / Azure / Snowflake. You should have excellent business and communication skills and be able to work with business owners to understand their data requirements and help them make data-related decisions using your data knowledge and experience.\n\nThe job might also require you to learn new tools and technologies fast, and you should have in-depth database concepts, ETL concepts, solid data querying, and debugging skills.\n\nYou will help to build and support efficient and stable data pipelines which can be easily maintained in the future using Azure Data Factory and Snowflake.\n\nYou will also be responsible of automating the data pipeline using cloud-based tools, testing, and clearly documenting the implementations.\n\nYou\u2019ll enjoy the flexibility to work remotely\n\nfrom anywhere within the U.S. as you take on some tough challenges.\n\nPrimary Responsibilities :\n\u2022 Engage in all phases of the development life cycle to lead the design, development and implementation of data interfaces supporting enterprise operations, reporting and analytics\n\u2022 Interact with technical and non-technical customers to understand requirements and implement relevant data solutions\n\u2022 Proactively and continuously identify, evaluate and recommend integration and delivery process improvements\n\u2022 Assist in the overall architecture of the analytics warehouse Design, and proactively provide inputs in designing, implementing, and automating the data flows\n\u2022 Developing ETL pipelines and data flows in and out of the data warehouse using a combination of Azure Data Factory and Snowflake toolsets\n\u2022 Developing idempotent ETL process design so that interrupted, incomplete, or failed processes can be rerun without errors using ADF dataflows and Pipelines\n\u2022 Ability to work in Snowflake Virtual Warehouses as needed in Snowflake and automate data pipelines using Snowpipe for tedious ETL problems\n\u2022 Capturing changes in data dimensions and maintaining versions of them using Stream sets in snowflake and scheduling them using Tasks\n\u2022 Optimize every step of the data movement not only limited to source and during travel but also when it's at rest in the database for accelerated responses\n\u2022 Must have the ability to build a highly efficient orchestrator that can schedule jobs, execute workflows, perform Data quality checks, and coordinate dependencies among tasks\n\u2022 Testing of ETL system code, data design, and pipelines and data flows. Root cause analysis on all processes and resolving production issues are also a part of the process and routine tests on databases and data flow and pipeline testing\n\u2022 Documenting the implementations, and test cases as well as for building deployment documents needed for CI / CD\n\nYou\u2019ll be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role as well as provide development for other roles you may be interested in.\n\nRequired Qualifications :\n\u2022 3+ years of experience in data warehousing concepts, ETL concepts, and working with ETL / Data quality tools on any ETL platform\n\u2022 3+ years of experience in writing complex SQL / Python queries, and performance tuning\n\u2022 1+ years of experience with Microsoft Azure cloud services and data warehouse environment\n\u2022 1+ years of experience with building, deploying, and troubleshooting data extraction and loading pipelines (ETL) using Azure Data Factory (ADF)\n\u2022 1+ years analyzing and executing data profiles\n\nPreferred Qualifications :\n\u2022 Azure or Snowflake Certification in database and data engineer roles\n\u2022 1+ years of experience with Adobe CJA (consumer journey analytics)\n\u2022 1+ years of experience developing a data solution on Snowflake Data Warehouse using snowflake continuous data pipelines with Snowpipe, Streams and Tasks\n\u2022 1+ years of experience working with databricks\n\u2022 1+ years of experience working with healthcare data\n\u2022 Experience is working with offshore teams (India etc.)\n\nCareers with UnitedHealthcare. Work with a Fortune 5 organization that\u2019s serving millions of people as we transform health care with bold ideas.\n\nBring your energy for driving change for the better. Help us improve health access and outcomes for everyone, as we work to advance health equity, connecting people with the care they need to feel their best.\n\nAs an industry leader, our commitment to improving lives is second to none.\n\nCalifornia, Colorado, Connecticut, Nevada, New York, Rhode Island, or Washington Residents Only : The salary range for California, Colorado, Connecticut, Nevada, New York, Rhode Island or Washington residents is $85,000 to $167,300.\n\nPay is based on several factors including but not limited to education, work experience, certifications, etc. In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements).\n\nNo matter where or when you begin a career with UnitedHealth Group, you\u2019ll find a far-reaching choice of benefits and incentives.\n\nAll employees working remotely will be required to adhere to UnitedHealth Group\u2019s Telecommuter Policy\n\nAt UnitedHealth Group, our mission is to help people live healthier lives and make the health system work better for everyone.\n\nWe believe everyone of every race, gender, sexuality, age, location and income deserves the opportunity to live their healthiest life.\n\nToday, however, there are still far too many barriers to good health which are disproportionately experienced by people of color, historically marginalized groups and those with lower incomes.\n\nWe are committed to mitigating our impact on the environment and enabling and delivering equitable care that addresses health disparities and improves health outcomes an enterprise priority reflected in our mission.\n\nDiversity creates a healthier atmosphere : UnitedHealth Group is an Equal Employment Opportunity / Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.\n\nUnitedHealth Group is a drug-free workplace. Candidates are required to pass a drug test before beginning employment.\n\nLast updated : 2023-04-22",
            "2023-04-22T00:00:00.000Z",
            [
                "You should have excellent business and communication skills and be able to work with business owners to understand their data requirements and help them make data-related decisions using your data knowledge and experience",
                "The job might also require you to learn new tools and technologies fast, and you should have in-depth database concepts, ETL concepts, solid data querying, and debugging skills",
                "3+ years of experience in data warehousing concepts, ETL concepts, and working with ETL / Data quality tools on any ETL platform",
                "3+ years of experience in writing complex SQL / Python queries, and performance tuning",
                "1+ years of experience with Microsoft Azure cloud services and data warehouse environment",
                "1+ years of experience with building, deploying, and troubleshooting data extraction and loading pipelines (ETL) using Azure Data Factory (ADF)",
                "1+ years analyzing and executing data profiles"
            ],
            [
                "California, Colorado, Connecticut, Nevada, New York, Rhode Island, or Washington Residents Only : The salary range for California, Colorado, Connecticut, Nevada, New York, Rhode Island or Washington residents is $85,000 to $167,300",
                "Pay is based on several factors including but not limited to education, work experience, certifications, etc",
                "In addition to your salary, UnitedHealth Group offers benefits such as, a comprehensive benefits package, incentive and recognition programs, equity stock purchase and 401k contribution (all benefits are subject to eligibility requirements)",
                "No matter where or when you begin a career with UnitedHealth Group, you\u2019ll find a far-reaching choice of benefits and incentives"
            ]
        ],
        [
            "Jobot",
            "FULLTIME",
            "Senior Snowflake Data Engineer",
            "https://www.dice.com/job-detail/2f22cbfe-0469-4eb7-a4cb-f028be9791b9",
            "Well established finance company hiring in San Diego!\n\nThis Jobot Job is hosted by: Jordan Goulding\nAre you a fit? Easy Apply now by clicking the \"Apply Now\" button and sending us your resume.\nSalary: $120,000 - $180,000 per year\n\nA bit about us:\n\nWe are a well established finance company issuing over $2 billion in small business loans to our customers, come join a team of top talent in the industry!\n\nWhy join us?\n\nCompetitive Pay\nHealth Benefits\nPTO\n401K\n\nJob Details\n\nWe are seeking a highly skilled and experienced Senior Snowflake Data Engineer to join our dynamic team in the FinTech industry. As a Senior Snowflake Data Engineer, you will be responsible for designing, developing, and maintaining our data warehouse and ETL processes, ensuring data accuracy, and implementing best practices for data management. You will work closely with our data scientists, business analysts, and other stakeholders to provide insights and drive business decisions.\n\nResponsibilities:\n\u2022 Design and develop our Snowflake data warehouse and ETL processes\n\u2022 Implement and maintain RBAC and SCD frameworks\n\u2022 Develop and maintain Snowflake SQL scripts and Snowflake utilities\n\u2022 Develop and maintain SnowSQL and SnowPipe scripts\n\nQualifications:\n\u2022 3+ years of experience as a Snowflake Data Engineer or related role\n\u2022 Strong experience with Snowflake, ETL, and data warehousing concepts\n\u2022 Strong experience with Snowflake SQL, RBAC, and SCD framework\n\u2022 Experience with Snowflake utilities such as SnowSQL and SnowPipe\n\nInterested in hearing more? Easy Apply now by clicking the \"Apply Now\" button.",
            "2023-04-19T12:30:14.000Z",
            [
                "3+ years of experience as a Snowflake Data Engineer or related role",
                "Strong experience with Snowflake, ETL, and data warehousing concepts",
                "Strong experience with Snowflake SQL, RBAC, and SCD framework",
                "Experience with Snowflake utilities such as SnowSQL and SnowPipe"
            ],
            [
                "Salary: $120,000 - $180,000 per year",
                "Competitive Pay",
                "Health Benefits",
                "PTO",
                "401K"
            ]
        ],
        [
            "KPMG US",
            "FULLTIME",
            "Senior Associate, Data Engineer",
            "https://www.linkedin.com/jobs/view/senior-associate-data-engineer-at-kpmg-us-3575903610",
            "Requisition Number: 102246 - 10\n\nDescription:\n\nKnown for being a great place to work and build a career, KPMG provides audit, tax and advisory services for organizations in today\u2019s most important industries. Our growth is driven by delivering real results for our clients. It\u2019s also enabled by our culture, which encourages individual development, embraces an inclusive environment, rewards innovative excellence and supports our communities. With qualities like those, it\u2019s no wonder we\u2019re consistently ranked among the best companies to work for by Fortune Magazine, Consulting Magazine, Working Mother Magazine, Diversity Inc. and others. If you\u2019re as passionate about your future as we are, join our team.\n\nKPMG is currently seeking a Senior Associate, Data Engineer to join our Growth and Strategy organization.\n\nResponsibilities:\n\u2022 Assess, capture, and translate complex business issues and solution requirements into structured technical tasks for the data engineering team, including rapid learning of industry standards and development of effective work stream plans\n\u2022 Work on the development of big data analytics and cloud-based applications to facilitate the work of data scientists and application developers\n\u2022 Design, build, launch, optimize and extend full-stack data and business intelligence solutions spanning extraction, storage, complex transformation, and visualization layers\n\u2022 Support big data environments that enable analytics solutions on a variety of big data platforms including assessing the usefulness of new technologies and advocating for their adoption\n\u2022 Serve as the primary point of contact for question and escalations related to data architecture and solutions applicable to business problems or use cases\n\nQualifications:\n\u2022 Minimum three years of recent experience working as a data engineer with advanced SQL skills\n\u2022 Bachelor's degree from an accredited college/university in a numerate subject such as Computer Science, Mathematics, Electrical Engineering, Statistics, or Science or equivalent practical experience\n\u2022 Experience creating and maintaining ETL/ELT pipelines that operate on a variety of sources such as APIs, FTP sites, cloud-based blob stores, databases including relational and non-relational, unstructured data; strong analytical, data profiling and problem-solving skills\n\u2022 Hands-on experience with Microsoft data and analytics platform such as Azure Data Factory, Microsoft SQL Server, Azure SQL Database, Azure SQL Managed Instance, Azure Cosmos DB, Azure Synapse Analytics, and Azure Databricks\n\u2022 Experience with SDLC methodologies, particularly Agile and project management tools, preferably Azure DevOps; prior experience working with operational programming tasks, such as version control, CI/CD, testing and quality assurance\n\u2022 Applicants must be currently authorized to work in the United States without the need for visa sponsorship now or in the future\n\nKPMG complies with all local/state regulations in regards to displaying salary ranges. If required, the salary range(s) are displayed below and are specifically for those potential hires who will perform work in or reside in the location(s) listed, if selected for the role. Any offered salary is determined based on internal equity, internal salary ranges, market-based salary ranges, applicant's skills and prior relevant experience, certain degrees and certifications (e.g. JD, technology), for example.\n\nSan Francisco / Santa Clara / Walnut Creek / San Jose / Los Angeles / Irvine / San Diego / Seattle / Bellevue Salary Range: Low: $88900 - High: $157600\n\nSacramento Salary Range: Low: $80800 - High: $143300\n\nAlbany Salary Range: Low: $76800 - High: $136100\n\nKPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an affirmative action-equal opportunity employer. KPMG complies with all applicable federal, state and local laws regarding recruitment and hiring. All qualified applicants are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, citizenship status, disability, protected veteran status, or any other category protected by applicable federal, state or local laws. The attached link contains further information regarding the firm's compliance with federal, state and local recruitment and hiring laws. No phone calls or agencies please.\n\nKPMG does not currently require partners or employees to be fully vaccinated or test negative for COVID-19 in order to go to KPMG offices, client sites or KPMG events, except when mandated by federal, state or local law. In some circumstances, clients also may require proof of vaccination or testing (e.g., to go to the client site).",
            "2023-04-21T10:28:42.000Z",
            [
                "Minimum three years of recent experience working as a data engineer with advanced SQL skills",
                "Bachelor's degree from an accredited college/university in a numerate subject such as Computer Science, Mathematics, Electrical Engineering, Statistics, or Science or equivalent practical experience",
                "Experience creating and maintaining ETL/ELT pipelines that operate on a variety of sources such as APIs, FTP sites, cloud-based blob stores, databases including relational and non-relational, unstructured data; strong analytical, data profiling and problem-solving skills",
                "Hands-on experience with Microsoft data and analytics platform such as Azure Data Factory, Microsoft SQL Server, Azure SQL Database, Azure SQL Managed Instance, Azure Cosmos DB, Azure Synapse Analytics, and Azure Databricks",
                "Experience with SDLC methodologies, particularly Agile and project management tools, preferably Azure DevOps; prior experience working with operational programming tasks, such as version control, CI/CD, testing and quality assurance",
                "Applicants must be currently authorized to work in the United States without the need for visa sponsorship now or in the future"
            ],
            [
                "Any offered salary is determined based on internal equity, internal salary ranges, market-based salary ranges, applicant's skills and prior relevant experience, certain degrees and certifications (e.g. JD, technology), for example",
                "San Francisco / Santa Clara / Walnut Creek / San Jose / Los Angeles / Irvine / San Diego / Seattle / Bellevue Salary Range: Low: $88900 - High: $157600",
                "Sacramento Salary Range: Low: $80800 - High: $143300",
                "Albany Salary Range: Low: $76800 - High: $136100",
                "KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package"
            ]
        ],
        [
            "Amwell",
            "FULLTIME",
            "Senior Data Engineer",
            "https://us.trabajo.org/job-640-20230423-50c109d031961ba90dd219bde963c86b",
            "Brief Overview:The Senior Data Engineer will manage, utilize, move, and transform data from our data center and cloud repositories to create reports for senior management and clients. This individual will conduct business analysis on data requests to ensure clarity on the need and proper data usage, adhering to security mandates. They will build various ETL pipelines, among the various tools in play, to surface data for consumption by our reporting tool. The Senior Data Engineer will prioritize competing requests from internal and external stakeholders, in addition to keeping the reporting infrastructure on par with new product functionality and release cycles. This individual will become a subject matter expert in data classification within the platform and utilize their expertise to identify the most efficient path to deliver data from A to B, as needed. Working hand in hand with Hosting, Analytics and Product, the Senior Data Engineer will be accountable for the delivery of accurate and timely reporting solutions.Core Responsibilities:Develop, build, deploy and maintain cloud data platform solutions.Design and support the database and table schemas for new and evolving sources of data being brought into the data warehouse.Monitor and troubleshoot performance issues.Define and promote the team''s design principles and best practices.Work with business teams to be able to define requirements for real time reporting.Qualifications:8+ years of development experience building data pipelines.5+ years of experience in architecture of modern data warehousing platforms using technologies such as Big Data, Cloud, and Kafka experience.Cloud experience - any cloud, preferably Bigquery, data flow, pub-sub, data fusion.Migration experience, utilizing GCP to move data from on-prem servers to the cloud. Good Python development for data transfers and extractions (ELT or ETL). Experience developing and deploying ETL solutions like Informatica or similar tools.Experience working within an agile development process (Scrum, Kanban, etc.).Familiarity with CI/CD concepts.Demonstrated proficiency in creating technical documentationUnderstanding of modern concepts (how new-gen DB is implemented - like how BQ/Redshift works?).Airflow, Dag development formatica or any ETL tool previous experience.Ability and experience in BI and Data Analysis and end-to-end development in data platform environments.Modern concepts (how new-gen DB is implemented - like how BQ/Redshift works?).Fix things before they break.Write excellent, fully tested code to build ETL /ELT data pipelines on Cloud.Candidate must have prior experience coordinating offshore teams and working in the onsite-offshore model.Bachelor''s Degree or equivalent experience is required. Preferred in Computer Science or related degree.Good to have - Health care domain experience.Additional informationShould you join Amwell and the Engineering team, you can expect:The development organization is a multi-disciplinary team of engineers dedicated to creating a state-of-the-art TeleHealth experience on every platform we can get our hands on. Our cross-functional teams follow a pragmatic Agile methodology as we balance feature requests, strategic initiatives, tech debt, and exciting partnerships on the path to delivering a market leading product to a quickly growing customer base. We work hand in hand with the whole Amwell organization to ensure that our product meets the needs of all of our users.Working at Amwell:Amwell is changing how care is delivered through online and mobile technology. We strive to make the hard work of healthcare look easy. In order to make this a reality, we look for people with a fast-paced, mission-driven mentality. We''re a culture that prides itself on quality, efficiency, smarts, initiative, creative thinking, and a strong work ethic. Our Core Values include One Team, Customer First, and Deliver Awesome. Customer First and Deliver Awesome are all about our product and services and how we strive to serve. As part of One Team, we operate the Amwell Cares program, which brings needed assistance to our communities, whether that be free healthcare for the underserved or for people affected by natural disasters, support for equality, honoring doctors and nurses, or annual Amwell-matched donations to food banks. Amwell aims to be a force for good for our employees, our clients, and our communities.Amwell cares deeply about and supports Diversity, Equity and Inclusion. These initiatives are highlighted and reflected within our Three DE&I Pillars - our Workplace, our Workforce and our Community.Amwell is a ''virtual first'' workplace, which means you can work from anywhere, coming together physically for ideation, collaboration and client meetings. We enable our employees with the tools, resources and opportunities to do their jobs effectively wherever they are Amwell has collaboration spaces in Boston, Tysons Corner, Portland, Woodland Hills, and Seattle.Unlimited Personal Time Off (Vacation time)401K matchCompetitive healthcare, dental and vision insurance plansPaid Parental Leave (Maternity and Paternity leave)Employee Stock Purchase ProgramFree access to Amwell''s Telehealth Services, SilverCloud and The Clinic by Cleveland Clinic''s second opinion programFree Subscription to the Calm AppTuition Assistance ProgramPet Insurance",
            "2023-04-23T19:11:43.000Z",
            [
                "Work with business teams to be able to define requirements for real time reporting.Qualifications:8+ years of development experience building data pipelines.5+ years of experience in architecture of modern data warehousing platforms using technologies such as Big Data, Cloud, and Kafka experience",
                "Cloud experience - any cloud, preferably Bigquery, data flow, pub-sub, data fusion",
                "Migration experience, utilizing GCP to move data from on-prem servers to the cloud",
                "Good Python development for data transfers and extractions (ELT or ETL)",
                "Experience developing and deploying ETL solutions like Informatica or similar tools",
                "Experience working within an agile development process (Scrum, Kanban, etc.).Familiarity with CI/CD concepts",
                "Demonstrated proficiency in creating technical documentation",
                "Understanding of modern concepts (how new-gen DB is implemented - like how BQ/Redshift works?).Airflow, Dag development formatica or any ETL tool previous experience",
                "Ability and experience in BI and Data Analysis and end-to-end development in data platform environments",
                "Modern concepts (how new-gen DB is implemented - like how BQ/Redshift works?).Fix things before they break",
                "Candidate must have prior experience coordinating offshore teams and working in the onsite-offshore model",
                "Bachelor''s Degree or equivalent experience is required",
                "Good to have - Health care domain experience"
            ],
            [
                "Unlimited Personal Time Off (Vacation time)401K match",
                "Competitive healthcare, dental and vision insurance plans",
                "Paid Parental Leave (Maternity and Paternity leave)Employee Stock Purchase ProgramFree access to Amwell''s Telehealth Services, SilverCloud and The Clinic by Cleveland Clinic''s second opinion program",
                "Free Subscription to the Calm AppTuition Assistance ProgramPet Insurance"
            ]
        ],
        [
            "Bristol Myers Squibb",
            "FULLTIME",
            "RDC Data Engineer",
            "https://jobs.alpfa.org/job/rdc-data-engineer/68898656/",
            "Working with Us\nChallenging. Meaningful. Life-changing. Those aren't words that are usually associated with a job. But working at Bristol Myers Squibb is anything but usual. Here, uniquely interesting work happens every day, in every department. From optimizing a production line to the latest breakthroughs in cell therapy, this is work that transforms the lives of patients, and the careers of those who do it. You'll get the chance to grow and thrive through opportunities uncommon in scale and scope, alongside high-achieving teams rich in diversity. Take your career farther than you thought possible.\n\nBristol Myers Squibb recognizes the importance of balance and flexibility in our work environment. We offer a wide variety of competitive benefits, services and programs that provide our employees with the resources to pursue their goals, both at work and in their personal lives. Read more: careers.bms.com/working-with-us\n\nWorkat the interface of pharma, genomics, and data engineering. The candidate will significantly contribute to the development of modern data services for BMS's research scientists.\n\nJob Description\n\nBristol Myers Squibb seeks a highly motivated Data Engineer to enable data integration efforts in support of data science and computational research efforts. The Data Engineer will be responsible for executing an ambitious digital strategy to support BMS's predictive science capabilities in Research. The successful candidate will partner closely with computational researcher teams,ITleadership, and various technical functions to design and deliver data solutions that streamline access to computing & data, and help scientists derive insight and value from their research.\n\nJob Functions\n\nThe role requires someone who can seamlessly mesh technical knowledge to help navigate R&D cloud, CoLo, andon-premisecomputing needs, including planning, infrastructure design, maintenance, and support.\n\nThe role will lead the development of infrastructure that enables interoperability and comparability of data sets derived from different technologies and biological systems in the context of integrative data analysis. The candidate will create and maintain optimal data pipeline architectures that enable scientific workflow and collaborate with interdisciplinary teams of data curators, software engineers, data scientists and computational biologists as we test new hypotheses through the novel integration of emerging research data types.\n\nThe work will combine careful resource planning and project management with hands-on data manipulation and implementation of data integration workflows.\n\nResponsibilities include, but are not limited to, the following:\n\u2022 Designing and developing an ETL infrastructure to load research data from multiple source systems using languages and frameworks such as Python, R, Docker, Airflow, Glue, etc.\n\u2022 Leading the design and implementation of data services solutions that may include relational, NoSQL and graph database components.\n\u2022 Collaborating with project managers, solution architects, infrastructure teams, and external vendors as needed to support successful delivery of technical solutions.\n\nJob Requirements\n\u2022 Bachelor's Degree with 8+ years of academic / industry experience or master's degree with 6+ years of academic / industry experience or PhD with 3+ years of academic / industry experience in an engineering or biology field or equivalent experience.\n\u2022 Demonstrated high proficiency with current software engineering methodologies, such as Agile SDLC approaches, distributed source code control, project management, issue tracking, and CI/CD tools and processes.\n\u2022 Excellent skills in an object-oriented programming language such as Python or R, and proficiency in SQL\n\u2022 High degree of proficiency in cloud computing\n\u2022 Solid understanding of container strategies such as Docker, Fargate, ECS and ECR.\n\u2022 Excellent skills and deep knowledge of databases such as Postgres, Elasticsearch, Redshift, and Aurora, including distributed database design, SQL vs. NoSQL, and database optimizations\n\u2022 Demonstrated high proficiency with current software engineering methodologies, such as Agile SDLC (Software Development Life Cycle) approaches, distributed source code control, project management, issue tracking, and CI/CD tools and processes.\n\u2022 Strong technical communication skills.\n\nThe starting compensation for this jobis a range from $114,000 - $159,000, plus incentive cash and stock opportunities (based on eligibility).\n\nThe starting pay rate takes into account characteristics of the job, such as required skills and the where the job is performed. Final individual compensation will be decided based on demonstrated experience.\n\nFor more on benefits, please visit Working With Us (bms.com).Eligibility for specific benefits listed on our BMS Careers site may vary based on the job and location.\n\n#LI-Hybrid\n\nIf you come across a role that intrigues you but doesn't perfectly line up with your resume, we encourage you to apply anyway. You could be one step away from work that will transform your life and career.\n\nUniquely Interesting Work, Life-changing Careers\nWith a single vision as inspiring as \"Transforming patients' lives through science\u2122 \", every BMS employee plays an integral role in work that goes far beyond ordinary. Each of us is empowered to apply our individual talents and unique perspectives in an inclusive culture, promoting diversity in clinical trials, while our shared values of passion, innovation, urgency, accountability, inclusion and integrity bring out the highest potential of each of our colleagues.\n\nOn-site Protocol\nPhysical presence at the BMS worksite or physical presence in the field is a necessary job function of this role, which the Company deems critical to collaboration, innovation, productivity, employee well-being and engagement, and it enhances the Company culture.\n\nCOVID-19 Information\nTo protect the safety of our workforce, customers, patients and communities, the policy of the Company requires all employees and workers in the U.S. and Puerto Rico to be fully vaccinated against COVID-19, unless they have received an exception based on an approved request for a medical or religious reasonable accommodation.Therefore, all BMS applicants seeking a role located in the U.S. and Puerto Rico must confirm that they have already received or are willing to receive the full COVID-19 vaccination by their start date as a qualification of the role and condition of employment.This requirement is subject to state and local law restrictions and may not be applicable to employees working in certain jurisdictions such as Montana. This requirement is also subject to discussions with collective bargaining representatives in the U.S.\n\nBMS is dedicated to ensuring that people with disabilities can excel through a transparent recruitment process, reasonable workplace accommodations/adjustments and ongoing support in their roles. Applicants can request a reasonable workplace accommodation/adjustment prior to accepting a job offer. If you require reasonable accommodations/adjustments in completing this application, or in any part of the recruitment process, direct your inquiries to adastaffingsupport@bms.com. Visit careers.bms.com/eeo-accessibility to access our complete Equal Employment Opportunity statement.\n\nBMS will consider for employment qualified applicants with arrest and conviction records, pursuant to applicable laws in your area.\n\nAny data processed in connection with role applications will be treated in accordance with applicable data privacy policies and regulations.",
            "2023-04-21T00:00:00.000Z",
            [
                "Bachelor's Degree with 8+ years of academic / industry experience or master's degree with 6+ years of academic / industry experience or PhD with 3+ years of academic / industry experience in an engineering or biology field or equivalent experience",
                "Excellent skills in an object-oriented programming language such as Python or R, and proficiency in SQL",
                "High degree of proficiency in cloud computing",
                "Solid understanding of container strategies such as Docker, Fargate, ECS and ECR",
                "Excellent skills and deep knowledge of databases such as Postgres, Elasticsearch, Redshift, and Aurora, including distributed database design, SQL vs",
                "NoSQL, and database optimizations",
                "Demonstrated high proficiency with current software engineering methodologies, such as Agile SDLC (Software Development Life Cycle) approaches, distributed source code control, project management, issue tracking, and CI/CD tools and processes",
                "Strong technical communication skills"
            ],
            [
                "The starting compensation for this jobis a range from $114,000 - $159,000, plus incentive cash and stock opportunities (based on eligibility)",
                "The starting pay rate takes into account characteristics of the job, such as required skills and the where the job is performed",
                "For more on benefits, please visit Working With Us (bms.com).Eligibility for specific benefits listed on our BMS Careers site may vary based on the job and location"
            ]
        ],
        [
            "Data Engineer",
            "FULLTIME",
            "TECHNOLOGY",
            "https://jobs.sandiegouniontribune.com/company/data-engineer-303066/job/technology-in-san-diego-ca-vopq1uapfe6ko8l0mlde195lsrcfhi/",
            "Description: Dsign, develop & integrte software for telecommunication systms. Int\u2019l Travel Req 25%. $155,397.00-$217,500.00/yr. EOE/AA m/f/disability/vets.",
            "2023-04-16T00:00:00.000Z",
            "No Qualifications data available",
            "No benefits data available"
        ],
        [
            "Data Engineer",
            "FULLTIME",
            "Accounting",
            "https://www.ihireaccounting.com/jobs/view/391661863",
            "ACCOUNTING Hutchinson and Bloodgood LLP has an opening for Tax Accountant at its office in San Diego, CA to prepare individual and business returns. $56680/yr. Mail resume: HR, 3205 S Dogwood Rd, El Centro, CA 92243. EOE\nrecblid h2pwcxxcjcbcb8pxoh3ocqlgcva0g6",
            "2023-04-23T17:02:00.000Z",
            "No Qualifications data available",
            "No benefits data available"
        ]
    ]
}